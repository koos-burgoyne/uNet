{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3899ee5d",
   "metadata": {},
   "source": [
    "# Week 12: U-Net\n",
    "## Convolutional Networks for Image Segmentation\n",
    "\n",
    "Olaf Ronneberger, Philipp Fischer, and Thomas Brox\n",
    "2015\n",
    "\n",
    "U-Net is a segmentation network whose architecture resembles the letter \"u\". The network \"contracts to capture context and a symmetric expanding path that enables precise localization\" (Ronnenberger et al. 2015).\n",
    "\n",
    "![unet.PNG](unet.PNG)\n",
    "\n",
    "The example provided here will use a dataset of images we chose to use is labeled to classify cars. There are two labels in these images ('car' and 'not car'). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b9e01",
   "metadata": {},
   "source": [
    "## Weighted Mask\n",
    "\n",
    "The use of the standard categorical cross-entropy loss function in U-net can be problematic for biomedical image segmentation:  \n",
    "* When using this loss function, the model is rewarded for classifying narrow cell boundaries as being in-cell pixels because they are close to many other in-cell pixels  \n",
    "* While this might not reduce the overall accuracy much, it reduces the usefulness of the segmentation result  \n",
    "* To mitigate this problem, the implementation from the paper includes a weight map that is used to add a weight value to the loss function.  \n",
    "\n",
    "This weight map is calculated as follows:\n",
    "\n",
    "$$\\Large\n",
    "w(x) = w_c(x) + w_0\\ \\cdot\\ exp\\ (-\\ \\frac{(d_1(x) + d_2(x))^2}{2 \\sigma^2})\n",
    "$$\n",
    "\n",
    "$$\\small  \n",
    "x\\ is\\ the\\ current\\ pixel \\\\  \n",
    "w_c\\ is\\ the\\ class\\ weighting\\ that\\ upweights\\ classes\\ differently \\\\  \n",
    "d_1\\ is\\ the\\ distance\\ to\\ the\\ nearest\\ cell\\ boundary \\\\  \n",
    "d_2\\ is\\ the\\ distance\\ to\\ the\\ second\\ nearest\\ cell\\ boundary \\\\  \n",
    "w_0\\ and\\ \\sigma\\ are\\ constants\\ that\\ scale\\ the\\ weighting $$  \n",
    "  \n",
    "### Example:  \n",
    "Let's implement this in code. First we need to develop a function that can generate random cells to simulate a microscopy image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.io import imshow\n",
    "from skimage.measure import label\n",
    "from scipy.ndimage.morphology import distance_transform_edt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def generate_random_circles(n = 50, d = 100, cell_size = 20):\n",
    "\n",
    "  \"\"\"\n",
    "  Generate a numpy array that is a simulated mask for an image of \n",
    "  cells where the in-cell pixels are 1 and background pixels are 0. \n",
    "  Used as prelimiary generator for unet_weight_map function.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  n:  int\n",
    "      Number of cells to be created\n",
    "  d:  int\n",
    "      Dictionary of weight classes.\n",
    "  Returns\n",
    "  -------\n",
    "  Numpy array\n",
    "      Simulated mask for a cell-image. A 2D array of shape (d,d).\n",
    "  \"\"\"\n",
    "  # Generate coordinates for random circles\n",
    "  circles = np.random.randint(0, d, (n, 3))\n",
    "  # Ensure radius (3rd col) is between d/3 and d\n",
    "  circles[:,2] = np.interp(circles[:,2], (circles[:,2].max(), circles[:,2].max()), (d/3,d))\n",
    "\n",
    "  # Generate background for simluated cell image \n",
    "  x = np.zeros((d, d), dtype=int)\n",
    "  \n",
    "  # Iterate over circles and insert into simulated image\n",
    "  for x0, y0, r in circles:\n",
    "    copy_x = deepcopy(x)\n",
    "    # Generate new 2D array with circle of pixels with value 1 for current circle\n",
    "    new_x = np.fromfunction(lambda x, y: ((x - x0)**2 + (y - y0)**2) <= (r/d*cell_size)**2, x.shape)\n",
    "    \n",
    "    # Add new circle to cell map\n",
    "    for i in range(1,len(new_x)-1):\n",
    "      for j in range(1,len(new_x[0])-1):\n",
    "        # Only add to cell map if neighbours are 0\n",
    "        if new_x[i][j] == True and x[i-1][j]==0 and x[i-1][j-1]==0 and x[i][j-1]==0 and x[i][j+1]==0 and x[i+1][j+1]==0 and x[i+1][j]==0 and x[i+1][j-1]==0 and x[i-1][j+1]==0:\n",
    "          copy_x[i][j] += new_x[i][j]\n",
    "    x += copy_x\n",
    "  \n",
    "  # Clip values above 1 to a max of 1\n",
    "  x = np.clip(x, 0, 1)\n",
    "\n",
    "  # Return simulated cell map\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa24245",
   "metadata": {},
   "source": [
    "Second we need to implement the weighting function that weights each pixel in the image based on it's distance to the nearest cell boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unet_weight_map(y, wc=None, w0 = 10, sigma = 5):\n",
    "\n",
    "  \"\"\"\n",
    "  Generate weight maps as specified in the U-Net paper\n",
    "  for boolean mask.\n",
    "  \n",
    "  \"U-Net: Convolutional Networks for Biomedical Image Segmentation\"\n",
    "  https://arxiv.org/pdf/1505.04597.pdf\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  mask: Numpy array\n",
    "      2D array of shape (image_height, image_width) representing binary mask\n",
    "      of objects.\n",
    "  wc: dict\n",
    "      Dictionary of weight classes.\n",
    "  w0: int\n",
    "      Border weight parameter.\n",
    "  sigma: int\n",
    "      Border width parameter.\n",
    "  Returns\n",
    "  -------\n",
    "  Numpy array\n",
    "      Training weights. A 2D array of shape (image_height, image_width).\n",
    "  \"\"\"\n",
    "  \n",
    "  # Grab labels from input\n",
    "  labels = label(y)\n",
    "  no_labels = labels == 0\n",
    "  label_ids = sorted(np.unique(labels))[1:]\n",
    "\n",
    "  # Check for circles in y\n",
    "  if len(label_ids) > 1:\n",
    "    # Calculate distances\n",
    "    distances = np.zeros((y.shape[0], y.shape[1], len(label_ids)))\n",
    "    for i, label_id in enumerate(label_ids):\n",
    "      distances[:,:,i] = distance_transform_edt(labels != label_id)\n",
    "\n",
    "    # Sort for nearest cell boundary distances\n",
    "    distances = np.sort(distances, axis=2)\n",
    "    d1 = distances[:,:,0]\n",
    "    d2 = distances[:,:,1]\n",
    "    \n",
    "    # Calculate second term in weighting equation\n",
    "    w = w0 * np.exp(-1/2*((d1 + d2) / sigma)**2) * no_labels\n",
    "\n",
    "    # Sum with class weights to produce weighted mask\n",
    "    if wc:\n",
    "      class_weights = np.zeros_like(y)\n",
    "      for k, v in wc.items():\n",
    "        class_weights[y == k] = v\n",
    "      w = w + class_weights\n",
    "  \n",
    "  # If no circles we passed in to the function, return zeros\n",
    "  else:\n",
    "    w = np.zeros_like(y)\n",
    "  \n",
    "  # Return weighted mask\n",
    "  return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1eb5b7",
   "metadata": {},
   "source": [
    "Let's generate a simulated cell image mask and then weight the pixels of that mask. You need to choose weighting values for the two classes, cell and non-cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated cell mask \n",
    "num_cells        = 60\n",
    "image_dimensions = 100\n",
    "cell_size        = 15\n",
    "y = generate_random_circles(num_cells, image_dimensions, cell_size)\n",
    "\n",
    "# Display cell mask\n",
    "plt.imshow(y)\n",
    "plt.title('Cell Image Mask')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Weighting for pixels assigned to the two classes\n",
    "class_weighting = {\n",
    "  # TODO: Choose weighting values for the classes in the cell mask - they are currently set to 0.\n",
    "  0: 0, # background\n",
    "  1: 0  # cells\n",
    "}\n",
    "\n",
    "print('Class Weighting:', class_weighting)\n",
    "\n",
    "# Generate weighted map\n",
    "w = unet_weight_map(y, class_weighting)\n",
    "\n",
    "# Display weighted mask\n",
    "plt.imshow(w)\n",
    "plt.title('Cell Image Weighted Mask')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b835a1c",
   "metadata": {},
   "source": [
    "**TO DO: Try adjusting the weighting of the class in the class_weighting dictionary (or other parameters) and see what impact that has on the final weighted mask.**\n",
    "\n",
    "# Implementing U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a3ce08",
   "metadata": {},
   "source": [
    "We use the double convolution quite a lot in this architecture so let's implement it as a reusable code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),  # 3x3 convolution\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7636f",
   "metadata": {},
   "source": [
    "Now let's devise the network using Pytorch functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.functional.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cb73a3",
   "metadata": {},
   "source": [
    "For a sanity check, let's just make sure that we are getting a prediction that is the shape we would expect.\n",
    "\n",
    "**QUESTION: Given the shape of the torch.randn() and the fact that we're using padding, what do you expect the output shape to be?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec710922",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((3, 1, 50, 50))\n",
    "model = UNET(in_channels=1, out_channels=1)\n",
    "preds = model(x)\n",
    "assert preds.shape == x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eff670",
   "metadata": {},
   "source": [
    "### Dataset Loader\n",
    "Now we'll prepare our data for training and testing using Pytorch's DataLoader which will pass in samples in “minibatches”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CarDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61b958",
   "metadata": {},
   "source": [
    "### Useful Utility Functions\n",
    "Some of the operations we perform can be functionalized to save on repetition. The two examples below are getting the data loaders and validating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd841210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_loaders(\n",
    "    train_dir,\n",
    "    train_maskdir,\n",
    "    val_dir,\n",
    "    val_maskdir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "):\n",
    "    train_ds = CarDataset(\n",
    "        image_dir=train_dir,\n",
    "        mask_dir=train_maskdir,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = CarDataset(\n",
    "        image_dir=val_dir,\n",
    "        mask_dir=val_maskdir,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def validation_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i,(x, y) in enumerate(loader,0):\n",
    "            x = x\n",
    "            y = y.unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            print(f\"{i/len(loader.dataset)*100.:.2f}%\",end='\\r')\n",
    "    print(f\"Got {num_correct}/{num_pixels} pixels with acc {num_correct/num_pixels*100:.2f}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21982c",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef081397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn):\n",
    "    losses = []\n",
    "    for batch_idx, (data, targets) in enumerate(loader):\n",
    "\n",
    "        # forward\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets.float().unsqueeze(1))\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Iter: {batch_idx+1}/{len(loader.dataset)}, Loss: {loss.item():.04f}\")\n",
    "        losses.append(loss.item())\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f8871",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Choose hyperparameters for the model. Experiment with the learning rate and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f58679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "IMAGE_HEIGHT = 160  # 1280 originally\n",
    "IMAGE_WIDTH = 240  # 1918 originally\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "TRAIN_IMG_DIR = os.path.join(cwd, \"data1k\", \"train_images\")\n",
    "TRAIN_MASK_DIR = os.path.join(cwd, \"data1k\", \"train_masks\")\n",
    "VAL_IMG_DIR = os.path.join(cwd, \"data1k\", \"val_images\")\n",
    "VAL_MASK_DIR = os.path.join(cwd, \"data1k\", \"val_masks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62d04e",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "The images in this dataset are quite large, so we will need to reduce the size in order to make the training time a little more reasonable. While this might sacrifice some accuracy due to compression, we can run more data through the model during training this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79951834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose(\n",
    "  [\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.Normalize(\n",
    "      mean=[0.0, 0.0, 0.0],\n",
    "      std=[1.0, 1.0, 1.0],\n",
    "      max_pixel_value=255.0,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "  ],\n",
    ")\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "  [\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.Normalize(\n",
    "      mean=[0.0, 0.0, 0.0],\n",
    "      std=[1.0, 1.0, 1.0],\n",
    "      max_pixel_value=255.0,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "  ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0acdec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = get_loaders(\n",
    "  TRAIN_IMG_DIR,\n",
    "  TRAIN_MASK_DIR,\n",
    "  VAL_IMG_DIR,\n",
    "  VAL_MASK_DIR,\n",
    "  BATCH_SIZE,\n",
    "  train_transform,\n",
    "  val_transforms,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e509552",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "First we need to write a weighting function that can be used to weight the mask for the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc97cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class car_weighting():\n",
    "  def __init__(self, wc, w0=10, sigma=5):\n",
    "    self.class_weights = wc\n",
    "    self.w0 = w0\n",
    "    self.sigma = sigma\n",
    "    \n",
    "  def weighted_mask(self, y):\n",
    "    tensor = False\n",
    "    if type(y)==torch.Tensor:\n",
    "      tensor=True\n",
    "      y = y[0][0]\n",
    "      y = y.numpy()\n",
    "  \n",
    "    # Grab labels from input\n",
    "    labels = label(y)\n",
    "\n",
    "    \"\"\" NOTE: We need to change these two lines to weight pixels on the inside edge of the car mask \"\"\"\n",
    "    no_labels = labels == 1\n",
    "    label_ids = sorted(np.unique(labels))\n",
    "\n",
    "    # Check for circles in y\n",
    "    if len(label_ids) > 1:\n",
    "      # Calculate distances\n",
    "      distances = np.zeros((y.shape[0], y.shape[1], len(label_ids)))\n",
    "      for i, label_id in enumerate(label_ids):\n",
    "        distances[:,:,i] = distance_transform_edt(labels != label_id)\n",
    "      \n",
    "      # Sort for nearest cell boundary distances\n",
    "      distances = np.sort(distances, axis=2)\n",
    "      d1 = distances[:,:,0]\n",
    "      d2 = distances[:,:,1]\n",
    "      \n",
    "      # Calculate second term in weighting equation\n",
    "      w = self.w0 * np.exp(-1/2*((d1+d2) / self.sigma)**2) * no_labels\n",
    "\n",
    "      # Sum with class weights to produce weighted mask\n",
    "      if self.class_weights:\n",
    "        class_weights = np.zeros_like(y)\n",
    "        for k, v in self.class_weights.items():\n",
    "          class_weights[y == k] = v\n",
    "        w = w + class_weights\n",
    "    \n",
    "    # If no circles we passed in to the function, return zeros\n",
    "    else:\n",
    "      w = np.zeros_like(y)\n",
    "    \n",
    "    # Return weighted mask\n",
    "    if tensor:\n",
    "      return torch.from_numpy(w)\n",
    "    else:\n",
    "      return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f754b7e2",
   "metadata": {},
   "source": [
    "Let's take a look at the weighed mask function and how it changes the mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = train_loader.dataset[0][1]\n",
    "plt.imshow(TF.ToPILImage()(mask).convert('L'), alpha=0.5)\n",
    "plt.title('Unweighted Mask')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "np_mask = mask.numpy()\n",
    "car_weight = {\n",
    "  # TODO: Choose weighting values for the classes in the cell mask - they are currently set to 0.\n",
    "  0: 1, # background\n",
    "  1: 2  # car\n",
    "}\n",
    "# TODO: Choose values for w0 and sigma:\n",
    "weight_fn = car_weighting(car_weight, w0=1, sigma=2)\n",
    "w_mask = weight_fn.weighted_mask(np_mask)\n",
    "\n",
    "plt.imshow(w_mask)\n",
    "plt.title('weighted mask')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fd3773",
   "metadata": {},
   "source": [
    "Next let's write a custom loss function that includes this weighted mask function like in the original paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb70d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, weighting_fn, pos_weight=1):\n",
    "      super().__init__()\n",
    "      self.pos_weight = pos_weight\n",
    "      self.weighting = weighting_fn\n",
    "\n",
    "    def forward(self, input, target):\n",
    "      epsilon = 10 ** -44\n",
    "      input_sig = torch.clamp(torch.sigmoid(input), epsilon, 1 - epsilon)\n",
    "      loss = -1 * self.weighting.weighted_mask(target) * (target*torch.log(input_sig) + (1-target)*torch.log(1-input_sig)) #(self.weighting.weighted_mask(target)) *\n",
    "      return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa55dfc2",
   "metadata": {},
   "source": [
    "Now we are ready to instantiate and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_weight = {\n",
    "  0: 0.1, # background\n",
    "  1: 0.5  # car\n",
    "}\n",
    "\n",
    "model = UNET(in_channels=3, out_channels=1)\n",
    "\n",
    "loss_fn = CustomBCEWithLogitsLoss(car_weighting(car_weight, w0=1, sigma=1)) #nn.BCEWithLogitsLoss()\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "losses = train_fn(train_loader, model, optimizer, loss_fn)\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1b77f",
   "metadata": {},
   "source": [
    "### Validating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy(val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e9c53",
   "metadata": {},
   "source": [
    "Let's visualise a couple randomly selected validation predictions and see how well the model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75da6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how many predictions do you want to visualize?\n",
    "num_predictions = 5\n",
    "\n",
    "for i in range(num_predictions):\n",
    "  # select a random image\n",
    "  idx = np.random.randint(0, len(val_loader.dataset))\n",
    "  # Get a prediction with that image\n",
    "  pred = torch.sigmoid(model(val_loader.dataset[idx][0].unsqueeze(0)))\n",
    "  pred = (pred > 0.5).float()\n",
    "  # display the prediction overlaid with the target mask\n",
    "  plt.imshow(pred.squeeze(), cmap=\"Greys\", alpha=0.6)\n",
    "  plt.title(f'{idx}')\n",
    "  plt.imshow(TF.ToPILImage()(val_loader.dataset[idx][1]).convert('L'), alpha=0.5)\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4fe176",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictions = 5\n",
    "\n",
    "for i in range(num_predictions):\n",
    "  # select a random image\n",
    "  idx = np.random.randint(0, len(val_loader.dataset))\n",
    "  # Get a prediction with that image\n",
    "  pred = torch.sigmoid(model(val_loader.dataset[idx][0].unsqueeze(0)))\n",
    "  pred = (pred > 0.5).float()\n",
    "  # display the prediction overlaid with the target mask\n",
    "  plt.title(f'{idx}')\n",
    "  plt.imshow(TF.ToPILImage()(pred.squeeze()*val_loader.dataset[idx][0]).convert('RGB'), alpha=1)\n",
    "  # plt.imshow(pred.squeeze(), 'Greys', alpha=1)\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ff675",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "Let's train the model on some new data (in a sense). We can augment the data that we feed to the model by performing transformations on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3507c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = A.Compose(\n",
    "#   [\n",
    "#     A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.Normalize(\n",
    "#       mean=[0.0, 0.0, 0.0],\n",
    "#       std=[2.0, 2.0, 2.0],\n",
    "#       max_pixel_value=255.0,\n",
    "#     ),\n",
    "#     ToTensorV2(),\n",
    "#   ],\n",
    "# )\n",
    "\n",
    "# val_transforms = A.Compose(\n",
    "#   [\n",
    "#     A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "#     A.HorizontalFlip(p=0.5),\n",
    "#     A.Normalize(\n",
    "#       mean=[0.0, 0.0, 0.0],\n",
    "#       std=[2.0, 2.0, 2.0],\n",
    "#       max_pixel_value=255.0,\n",
    "#     ),\n",
    "#     ToTensorV2(),\n",
    "#   ],\n",
    "# )\n",
    "\n",
    "# train_loader, val_loader = get_loaders(\n",
    "#   TRAIN_IMG_DIR,\n",
    "#   TRAIN_MASK_DIR,\n",
    "#   VAL_IMG_DIR,\n",
    "#   VAL_MASK_DIR,\n",
    "#   BATCH_SIZE,\n",
    "#   train_transform,\n",
    "#   val_transforms,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22bbb7",
   "metadata": {},
   "source": [
    "Now we continue training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce926d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_fn(train_loader, model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0e88a",
   "metadata": {},
   "source": [
    "Take a look at some of the model predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: how many predictions do you want to visualize?\n",
    "# num_predictions = 3\n",
    "\n",
    "# for i in range(num_predictions):\n",
    "#   # select a random image\n",
    "#   idx = np.random.randint(0, len(val_loader.dataset))\n",
    "#   # Get a prediction with that image\n",
    "#   pred = torch.sigmoid(model(val_loader.dataset[idx][0].unsqueeze(0)))\n",
    "#   pred = (pred > 0.5).float()\n",
    "#   # display the prediction overlaid with the target mask\n",
    "#   plt.imshow(pred.squeeze(), cmap=\"Greys\", alpha=0.6)\n",
    "#   plt.imshow(TF.ToPILImage()(val_loader.dataset[idx][1]).convert('L'), alpha=0.5)\n",
    "#   plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
