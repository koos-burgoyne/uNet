{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3899ee5d",
   "metadata": {},
   "source": [
    "# Week 12: U-Net\n",
    "## Convolutional Networks for Image Segmentation\n",
    "\n",
    "Olaf Ronneberger, Philipp Fischer, and Thomas Brox\n",
    "2015\n",
    "\n",
    "U-Net is a segmentation network whose architecture resembles the letter \"u\". The network \"contracts to capture context and a symmetric expanding path that enables precise localization\" (Ronnenberger et al. 2015).\n",
    "\n",
    "![unet.PNG](unet.PNG)\n",
    "\n",
    "The example provided here will use a dataset of images we chose to use is labeled to classify cars. There are two labels in these images ('car' and 'not car'). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b9e01",
   "metadata": {},
   "source": [
    "## Weighted Mask\n",
    "\n",
    "The use of the standard categorical cross-entropy loss function in U-net can be problematic for biomedical image segmentation:  \n",
    "* When using this loss function, the model is rewarded for classifying narrow cell boundaries as being in-cell pixels because they are close to many other in-cell pixels  \n",
    "* While this might not reduce the overall accuracy much, it reduces the usefulness of the segmentation result  \n",
    "* To mitigate this problem, the implementation from the paper includes a weight map that is used to add a weight value to the loss function.  \n",
    "\n",
    "This weight map is calculated as follows:\n",
    "\n",
    "$$\\Large\n",
    "w(x) = w_c(x) + w_0\\ \\cdot\\ exp\\ (-\\ \\frac{(d_1(x) + d_2(x))^2}{2 \\sigma^2})\n",
    "$$\n",
    "\n",
    "$$\\small  \n",
    "x\\ is\\ the\\ current\\ pixel \\\\  \n",
    "w_c\\ is\\ the\\ class\\ weighting\\ that\\ upweights\\ classes\\ differently \\\\  \n",
    "d_1\\ is\\ the\\ distance\\ to\\ the\\ nearest\\ cell\\ boundary \\\\  \n",
    "d_2\\ is\\ the\\ distance\\ to\\ the\\ second\\ nearest\\ cell\\ boundary \\\\  \n",
    "w_0\\ and\\ \\sigma\\ are\\ constants\\ that\\ scale\\ the\\ weighting $$  \n",
    "  \n",
    "### Example:  \n",
    "Let's implement this in code. First we need to develop a function that can generate random cells to simulate a microscopy image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.io import imshow\n",
    "from skimage.measure import label\n",
    "from scipy.ndimage.morphology import distance_transform_edt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def generate_random_circles(n = 50, d = 100, cell_size = 20):\n",
    "\n",
    "  \"\"\"\n",
    "  Generate a numpy array that is a simulated mask for an image of \n",
    "  cells where the in-cell pixels are 1 and background pixels are 0. \n",
    "  Used as prelimiary generator for unet_weight_map function.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  n:  int\n",
    "      Number of cells to be created\n",
    "  d:  int\n",
    "      Dictionary of weight classes.\n",
    "  Returns\n",
    "  -------\n",
    "  Numpy array\n",
    "      Simulated mask for a cell-image. A 2D array of shape (d,d).\n",
    "  \"\"\"\n",
    "  # Generate coordinates for random circles\n",
    "  circles = np.random.randint(0, d, (n, 3))\n",
    "  # Ensure radius (3rd col) is between d/3 and d\n",
    "  circles[:,2] = np.interp(circles[:,2], (circles[:,2].max(), circles[:,2].max()), (d/3,d))\n",
    "\n",
    "  # Generate background for simluated cell image \n",
    "  x = np.zeros((d, d), dtype=int)\n",
    "  \n",
    "  # Iterate over circles and insert into simulated image\n",
    "  for x0, y0, r in circles:\n",
    "    copy_x = deepcopy(x)\n",
    "    # Generate new 2D array with circle of pixels with value 1 for current circle\n",
    "    new_x = np.fromfunction(lambda x, y: ((x - x0)**2 + (y - y0)**2) <= (r/d*cell_size)**2, x.shape)\n",
    "    \n",
    "    # Add new circle to cell map\n",
    "    for i in range(1,len(new_x)-1):\n",
    "      for j in range(1,len(new_x[0])-1):\n",
    "        # Only add to cell map if neighbours are 0\n",
    "        if new_x[i][j] == True and x[i-1][j]==0 and x[i-1][j-1]==0 and x[i][j-1]==0 and x[i][j+1]==0 and x[i+1][j+1]==0 and x[i+1][j]==0 and x[i+1][j-1]==0 and x[i-1][j+1]==0:\n",
    "          copy_x[i][j] += new_x[i][j]\n",
    "    x += copy_x\n",
    "  \n",
    "  # Clip values above 1 to a max of 1\n",
    "  x = np.clip(x, 0, 1)\n",
    "\n",
    "  # Return simulated cell map\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa24245",
   "metadata": {},
   "source": [
    "Second we need to implement the weighting function that weights each pixel in the image based on it's distance to the nearest cell boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad58a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unet_weight_map(y, wc=None, w0 = 10, sigma = 5):\n",
    "\n",
    "  \"\"\"\n",
    "  Generate weight maps as specified in the U-Net paper\n",
    "  for boolean mask.\n",
    "  \n",
    "  \"U-Net: Convolutional Networks for Biomedical Image Segmentation\"\n",
    "  https://arxiv.org/pdf/1505.04597.pdf\n",
    "  \n",
    "  Parameters\n",
    "  ----------\n",
    "  mask: Numpy array\n",
    "      2D array of shape (image_height, image_width) representing binary mask\n",
    "      of objects.\n",
    "  wc: dict\n",
    "      Dictionary of weight classes.\n",
    "  w0: int\n",
    "      Border weight parameter.\n",
    "  sigma: int\n",
    "      Border width parameter.\n",
    "  Returns\n",
    "  -------\n",
    "  Numpy array\n",
    "      Training weights. A 2D array of shape (image_height, image_width).\n",
    "  \"\"\"\n",
    "  \n",
    "  # Grab labels from input\n",
    "  labels = label(y)\n",
    "  no_labels = labels == 0\n",
    "  label_ids = sorted(np.unique(labels))[1:]\n",
    "\n",
    "  # Check for circles in y\n",
    "  if len(label_ids) > 1:\n",
    "    # Calculate distances\n",
    "    distances = np.zeros((y.shape[0], y.shape[1], len(label_ids)))\n",
    "    for i, label_id in enumerate(label_ids):\n",
    "      distances[:,:,i] = distance_transform_edt(labels != label_id)\n",
    "\n",
    "    # Sort for nearest cell boundary distances\n",
    "    distances = np.sort(distances, axis=2)\n",
    "    d1 = distances[:,:,0]\n",
    "    d2 = distances[:,:,1]\n",
    "    \n",
    "    # Calculate second term in weighting equation\n",
    "    w = w0 * np.exp(-1/2*((d1 + d2) / sigma)**2) * no_labels\n",
    "\n",
    "    # Sum with class weights to produce weighted mask\n",
    "    if wc:\n",
    "      class_weights = np.zeros_like(y)\n",
    "      for k, v in wc.items():\n",
    "        class_weights[y == k] = v\n",
    "      w = w + class_weights\n",
    "  \n",
    "  # If no circles we passed in to the function, return zeros\n",
    "  else:\n",
    "    w = np.zeros_like(y)\n",
    "  \n",
    "  # Return weighted mask\n",
    "  return w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1eb5b7",
   "metadata": {},
   "source": [
    "Let's generate a simulated cell image mask and then weight the pixels of that mask. You need to choose weighting values for the two classes, cell and non-cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840b9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated cell mask \n",
    "num_cells        = 60\n",
    "image_dimensions = 100\n",
    "cell_size        = 15\n",
    "y = generate_random_circles(num_cells, image_dimensions, cell_size)\n",
    "\n",
    "# Display cell mask\n",
    "plt.imshow(y)\n",
    "plt.title('Cell Image Mask')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Weighting for pixels assigned to the two classes\n",
    "class_weighting = {\n",
    "  # TODO: Choose weighting values for the classes in the cell mask - they are currently set to 0.\n",
    "  0: 0, # background\n",
    "  1: 0  # cells\n",
    "}\n",
    "\n",
    "print('Class Weighting:', class_weighting)\n",
    "\n",
    "# Generate weighted map\n",
    "w = unet_weight_map(y, class_weighting)\n",
    "\n",
    "# Display weighted mask\n",
    "plt.imshow(w)\n",
    "plt.title('Cell Image Weighted Mask')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b835a1c",
   "metadata": {},
   "source": [
    "**TO DO: Try adjusting the weighting of the class in the class_weighting dictionary (or other parameters) and see what impact that has on the final weighted mask.**\n",
    "\n",
    "# Implementing U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a3ce08",
   "metadata": {},
   "source": [
    "We use the double convolution quite a lot in this architecture so let's implement it as a reusable code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054f2211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),  # 3x3 convolution\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7636f",
   "metadata": {},
   "source": [
    "Now let's devise the network using Pytorch functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced8572",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNET(nn.Module):\n",
    "    def __init__(\n",
    "            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n",
    "    ):\n",
    "        super(UNET, self).__init__()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Down part of UNET\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Up part of UNET\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(\n",
    "                    feature*2, feature, kernel_size=2, stride=2,\n",
    "                )\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.functional.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cb73a3",
   "metadata": {},
   "source": [
    "For a sanity check, let's just make sure that we are getting a prediction that is the shape we would expect.\n",
    "\n",
    "**QUESTION: Given the shape of the torch.randn() and the fact that we're using padding, what do you expect the output shape to be?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec710922",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((3, 1, 50, 50))\n",
    "model = UNET(in_channels=1, out_channels=1)\n",
    "preds = model(x)\n",
    "assert preds.shape == x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eff670",
   "metadata": {},
   "source": [
    "### Dataset Loader\n",
    "Now we'll prepare our data for training and testing using Pytorch's DataLoader which will pass in samples in “minibatches”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a0d284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CarDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.image_dir, self.images[index])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[index].replace(\".jpg\", \"_mask.gif\"))\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentations = self.transform(image=image, mask=mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d61b958",
   "metadata": {},
   "source": [
    "### Useful Utility Functions\n",
    "Some of the operations we perform can be functionalized to save on repetition. The two examples below are getting the data loaders and validating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd841210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_loaders(\n",
    "    train_dir,\n",
    "    train_maskdir,\n",
    "    val_dir,\n",
    "    val_maskdir,\n",
    "    batch_size,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "):\n",
    "    train_ds = CarDataset(\n",
    "        image_dir=train_dir,\n",
    "        mask_dir=train_maskdir,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    val_ds = CarDataset(\n",
    "        image_dir=val_dir,\n",
    "        mask_dir=val_maskdir,\n",
    "        transform=val_transform,\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def validation_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x\n",
    "            y = y.unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "    print(f\"Got {num_correct}/{num_pixels} pixels with acc {num_correct/num_pixels*100:.2f}\")\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21982c",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef081397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader, model, optimizer, loss_fn):\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(loader):\n",
    "\n",
    "        # forward\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets.float().unsqueeze(1))\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Iter: {batch_idx+1}/{len(loader.dataset)}, Loss: {loss.item():.04f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f8871",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Choose hyperparameters for the model. Experiment with the learning rate and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f58679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "# TODO: choose values\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "IMAGE_HEIGHT = 160  # 1280 originally\n",
    "IMAGE_WIDTH = 240  # 1918 originally\n",
    "LOAD_MODEL = False\n",
    "\n",
    "cwd = os.getcwd() #.replace(\"\\\\\",\"/\")\n",
    "\n",
    "TRAIN_IMG_DIR = os.path.join(cwd, \"data\", \"train_images\")\n",
    "TRAIN_MASK_DIR = os.path.join(cwd, \"data\", \"train_masks\")\n",
    "VAL_IMG_DIR = os.path.join(cwd, \"data\", \"val_images\")\n",
    "VAL_MASK_DIR = os.path.join(cwd, \"data\", \"val_masks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd62d04e",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "The images in this dataset are quite large, so we will need to reduce the size in order to make the training time a little more reasonable. While this might sacrifice some accuracy due to compression, we can run more data through the model during training this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79951834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose(\n",
    "  [\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.Normalize(\n",
    "      mean=[0.0, 0.0, 0.0],\n",
    "      std=[1.0, 1.0, 1.0],\n",
    "      max_pixel_value=255.0,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "  ],\n",
    ")\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "  [\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.Normalize(\n",
    "      mean=[0.0, 0.0, 0.0],\n",
    "      std=[1.0, 1.0, 1.0],\n",
    "      max_pixel_value=255.0,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "  ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e509552",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "**Question: Why do we use the BCEWithLogitsLoss function for training the model?**  \n",
    "The documentation is [here](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html).\n",
    "\n",
    "**Question: Given th authors of the original U-Net paper chose SGD, is Adam the best optimizer for this implementation?**  \n",
    "Consider trying [a couple of different ones](https://pytorch.org/docs/stable/optim.html#algorithms) and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc97cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNET(in_channels=3, out_channels=1)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_loader, val_loader = get_loaders(\n",
    "  TRAIN_IMG_DIR,\n",
    "  TRAIN_MASK_DIR,\n",
    "  VAL_IMG_DIR,\n",
    "  VAL_MASK_DIR,\n",
    "  BATCH_SIZE,\n",
    "  train_transform,\n",
    "  val_transforms,\n",
    ")\n",
    "\n",
    "train_fn(train_loader, model, optimizer, loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1b77f",
   "metadata": {},
   "source": [
    "### Validating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed2320",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_accuracy(val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175e9c53",
   "metadata": {},
   "source": [
    "Let's visualise a couple randomly selected validation predictions and see how well the model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75da6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how many predictions do you want to visualize?\n",
    "num_predictions = 3\n",
    "\n",
    "for i in range(num_predictions):\n",
    "  # select a random image\n",
    "  idx = np.random.randint(0, len(val_loader.dataset))\n",
    "  # Get a prediction with that image\n",
    "  pred = torch.sigmoid(model(val_loader.dataset[idx][0].unsqueeze(0)))\n",
    "  pred = (pred > 0.5).float()\n",
    "  # display the prediction overlaid with the target mask\n",
    "  plt.imshow(pred.squeeze(), cmap=\"Greys\", alpha=0.6)\n",
    "  plt.imshow(TF.ToPILImage()(val_loader.dataset[idx][1]).convert('L'), alpha=0.5)\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ff675",
   "metadata": {},
   "source": [
    "**Question: What you do notice about the predicted mask that is less than optimal?**  \n",
    "\n",
    "### Data Augmentation\n",
    "Let's train the model on some new data (in a sense). We can augment the data that we feed to the model by performing horizontal flips on the images of the cars, or even changing the distribution of the normalized values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3507c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "  [\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(\n",
    "      mean=[0.0, 0.0, 0.0],\n",
    "      std=[2.0, 2.0, 2.0],\n",
    "      max_pixel_value=255.0,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "  ],\n",
    ")\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "  [\n",
    "    A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(\n",
    "      mean=[0.0, 0.0, 0.0],\n",
    "      std=[2.0, 2.0, 2.0],\n",
    "      max_pixel_value=255.0,\n",
    "    ),\n",
    "    ToTensorV2(),\n",
    "  ],\n",
    ")\n",
    "\n",
    "train_loader, val_loader = get_loaders(\n",
    "  TRAIN_IMG_DIR,\n",
    "  TRAIN_MASK_DIR,\n",
    "  VAL_IMG_DIR,\n",
    "  VAL_MASK_DIR,\n",
    "  BATCH_SIZE,\n",
    "  train_transform,\n",
    "  val_transforms,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a22bbb7",
   "metadata": {},
   "source": [
    "Now continue training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce926d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn(train_loader, model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0e88a",
   "metadata": {},
   "source": [
    "Take a look at some of the model predictions.  \n",
    "  \n",
    "**Question: How has the model performance changed and why?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207d5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: how many predictions do you want to visualize?\n",
    "num_predictions = 3\n",
    "\n",
    "for i in range(num_predictions):\n",
    "  # select a random image\n",
    "  idx = np.random.randint(0, len(val_loader.dataset))\n",
    "  # Get a prediction with that image\n",
    "  pred = torch.sigmoid(model(val_loader.dataset[idx][0].unsqueeze(0)))\n",
    "  pred = (pred > 0.5).float()\n",
    "  # display the prediction overlaid with the target mask\n",
    "  plt.imshow(pred.squeeze(), cmap=\"Greys\", alpha=0.6)\n",
    "  plt.imshow(TF.ToPILImage()(val_loader.dataset[idx][1]).convert('L'), alpha=0.5)\n",
    "  plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cefe75",
   "metadata": {},
   "source": [
    "**As an optional final task, try training a new model on variations of augmented data and see how the performance changes with different augmentations.** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
